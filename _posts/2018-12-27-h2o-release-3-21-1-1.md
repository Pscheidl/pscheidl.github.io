---
title: "H2O Christmas release 2018 - a digest"
published: true
author: pavel_pscheidl
categories:
  - H2O-3
tags:
  - H2O.ai
  - H2O-3
  - Machine learning
  - AI
---

There were two releases shortly after each other. First, on December 21st, there was a minor (fix) release [3.22.0.3](https://h2o-release.s3.amazonaws.com/h2o/rel-xia/3/index.html). Immediately followed by a more major release (but still on 3.22 branch) codename `Xu`, named after mathematician [Jinchao Xu](https://en.wikipedia.org/wiki/Jinchao_Xu), whose work is focused on deep neural networks, besides many other fields of research.

Of course, the new `3.22.1.1` release with codename `Xu` contains all the fixes present in previous fix release `3.22.0.3`. The following points are highligting the most impactful changes. For a complete list of changes, fixes and improvements, please read the [recent changes section](https://github.com/h2oai/h2o-3/blob/master/Changes.md).

## Support for CDH 6.x

H<sub>2</sub>O now declares support for Cloudera distributions based on Hadoop 3. This includes releases by Cloudera `CDH 6.0`, `CDH 6.1`. Support for releases by Hortonworks `HDP 3.0` and `HDP 3.1`are going to be added shortly in one of the releases to follow.

## Partial dependence plots can be exported

The API to plot partial dependence graphs has been enhanced with the ability to directly export (save to disk) the plots. For both R and Python API, a new parameter named `save_to_file` has been added. The parameter accepts a string representing a path on a filesystem.

### Python
The existing method `partial_plot` callable on any `Model` able has been modified by adding the new `save_to_file` parameter. All the plots are saved into one image file.
```python
model.partial_plot(data = data,cols = ['AGE', 'RACE', 'DCAPS'],server = True, plot = True, save_to_file = "/home/username/pdp.png")
```

### R

H<sub>2</sub>O provides function `h2o.partialPlot` in R to create the plots. A new optional parameter named `save_to` has been introduced for this function. In R, different plots are traditionally represented by a separate pointer to a separate image. To honor this contract in H<sub>2</sub>O R API, the plots are not saved into one file, but one file is created per each feature.

```R
h2o.partialPlot(object = model, data = data, save_to = "/home/username/pdp")
```

If the `model` mentioned in the showcase above has been trained using a set of three features: [`Age`, `RACE`, `DCAPS`], then H2O is going to save three files onto the filesystem:

```plain
- /home/username/pdp_AGE.png
- /home/username/pdp_RACE.png
- /home/username/pdp_DCAPS.png
```

That is the reason why the string containing the filesystem path does not include `png` suffix. It is added automatically. If the suffix is added accidentally, it is stripped by H<sub>2</sub>O automatically.

## Monotonicity constraints for GBM

H<sub>2</sub>O users now have the ability to affect GBM splits by stating monotonic relationships of a feature to the predicted variable. Any subset of features used during model training phase might be restricted with monotonicity constraints.

### R

A new argument named `monotone_constraints` has been added to `h2o.gbm(..)`  function. This argument is optional and accepts a list of `key:value`. The key is the name of the feature and value is one of `{-1,0,1}`, where `-1` represents decreasing constraint, 0 represents no constraint and 1 represents increasing constraint.
```R
features <- ("Origin", "Dest","Distance")
features.constraints <- c(1, 0, -1)
monotonicity.constraints <- setNames(features.constraints, features)
gbm.model <- h2o.gbm(y = "IsDepDelayed",
                         training_frame = training_frame,
                         monotone_constraints = monotonicity.constraints,
                         validation_frame = validation_frame)
```

### Python

A new argument named `monotone_constraints` has been added to the `H2OXGBoostEstimator`'s constructor. This argument is optional and accepts a list of `key:value`. The key is the name of the feature and value is one of `{-1,0,1}`, where `-1` represents decreasing constraint, 0 represents no constraint and 1 represents increasing constraint.
```python
feature_names = ['MedInc', 'AveOccup', 'HouseAge']
monotone_constraints = {"MedInc": 1, "AveOccup": -1, "HouseAge": 1}
xgb_mono = H2OXGBoostEstimator(monotone_constraints = monotone_constraints)
xgb_mono.train(x = feature_names, y = "target", training_frame = train, validation_frame = test)
```

Full Python demo is available on [H<sub>2</sub>O GitHub](https://github.com/h2oai/h2o-3/blob/master/h2o-py/demos/H2O_tutorial_gbm_monotonicity.ipynb).

## AutoML performance improvement

In version `3.22.0.3`, the performance of all models in the AutoML run has been improved. AutoML previously automatically partitioned the training set, setting aside 10% of the data to be used for an early stopping. This dataset was not being used, so now AutoML simply uses the full training dataset to train the models, leading to better model performance. All details, including the actual fixes done, are to be found in [PUBDEV-6079 JIRA](https://0xdata.atlassian.net/browse/PUBDEV-6079).

## Custom metrics for early stopping

Specification for custom stoppic metrics is now supported for GBM, DRF and GLM algorithms.

There are two new stopping criteria:

- `custom` - for custom metric functions where "less is better", it is expected that the lower bound is 0
- `custom_increasing` - for custom metric functions where "more is better"


### Python

Custom stopping function

```python

 def custom_stopping_metric_function():
     return h2o.upload_custom_metric(CustomMaeFunc, func_name = "mae", func_file = "mm_mae.py")


model_actual = H2OGradientBoostingEstimator(model_id="prostate", ntrees = 10, max_depth = 5,
                                                 score_each_iteration = True,
                                                 custom_metric_func = custom_stopping_metric_function(),
                                                 stopping_metric = "custom",
                                                 stopping_tolerance = 0.1,
                                                 stopping_rounds = 3)
     model_actual.train(y = "AGE", x = ftrain.names, training_frame = ftrain, validation_frame = fvalid)
```

## Exporting checkpoints in AutoML

Throughout the AutoML experiments and Grid searches, resulting models can now be checkpointed. By specifying `export_checkpoints_dir` value, which is a string pointing to a directory int he filesystem, checkpoints are saved automatically as new models are created.

### AutoML Python
```python
     model = H2OAutoML(project_name = "ExampleProject", stopping_rounds = 3, export_checkpoints_dir = "/home/username/example/checkpoints")
     model.train(y = "CAPSULE", training_frame = training_frame)
```

### Grid Search Python
```python
    air_grid = H2OGridSearch(H2OGradientBoostingEstimator, hyper_params = hyper_parameters, search_criteria = search_crit)
    air_grid.train(x = ["Origin", "Distance"], y = "IsDepDelayed", training_frame = training_frame, export_checkpoints_dir = checkpoints_dir)
```

### AutoML R

```R
    model <- h2o.automl(y = y,
                         training_frame = train,
                         project_name = "AutoMLTest", 
                         export_checkpoints_dir = "/home/username/example/checkpoints")
```

### Grid search R

```R
grid = h2o.grid("glm", grid_id = "glm_grid_cars_test", x = predictors, y = "economy", training_frame = train,
                            family = "gaussian", hyper_params = hyper_params, export_checkpoints_dir = "checkpoints_dir")
```